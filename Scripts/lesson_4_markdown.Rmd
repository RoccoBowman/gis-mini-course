---
title: "Practical 4: Loops and Clusters"
author: "Rocco Bowman"
date: "5/31/2021"
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/bowma/Desktop/GIS Mini-course Summer 2021")
getwd()
```

## Introduction

Perhaps the greatest benefit of using a programming language as opposed to a point-and-click GIS software is the ability to iterate the same commands over and over, to different data sets or to different parts of the same data set. Iterating through **for loops** can also help you quickly and transparently test different parameters (such as number of clusters) against each other and even find an ideal number of clusters.

We can also automate whole chunks of code with functions to make our jobs easier in specific tasks.

## 4.1 K-means

K-means is a well-known algorithm in any kind of data analysis and is a good place to start thinking about spatial clustering and how **for loops** can automate cluster analysis. K-means is an algorithm that seeks to group data into reasonable clusters based on their location which, in standard planar space, is their values on two variables.

We will start by loading the **sf** and **tidyverse** packages, loading in our Airbnb data for Chicago (and dropping NA values which will cause us trouble later), and plotting the first several variables maps.

```{r}
library(sf)
library(tidyverse)

airbnb <- st_read("Day 4/data/airbnb.shp") %>%
  drop_na()

plot(airbnb)
```

Next, running the K-means algorithm on our data is quite simple. We will feed the coordinates of the centroids of our polygons as the first argument which is what K-means deals with---points---and specify 3 clusters or centers. Then we can plot the empty geometry of the airbnb object filled with the cluster identifications from our km object.

```{r}
km <- kmeans(st_coordinates(st_centroid(airbnb)), centers = 3)
plot(st_geometry(airbnb), col = km$cluster)
```

Maybe 3 spatially contiguous clusters is enough to satisfy some part of an analysis but since clustering is about optimizing each group to be *most disimilar* to their neighbors, it makes sense to test a range of values of *k* to find which number of clusters does the best job of it.

We can easily build a **for loop** around our initial code to test every *k* value 1 to 10 and then store that information in a master list for comparison as an elbow plot. We are specifically looking at the *within cluster sum of squares* to test for the variance within each cluster.

**Note: your plot might look a little different depending on which point K-means starts**

```{r warning=FALSE}
all_wss <- tibble() # make an empty master list to store variance info
for (k in 1:10) { # iterate where k will equal 1 to 10
  km <- kmeans(st_coordinates(st_centroid(airbnb)), centers = k) # iterator becomes centers argument
  summary <- tibble(k = k, twss = km$tot.withinss) # store this run's info
  all_wss <- bind_rows(all_wss,summary) # store this run's info into the master list
}

plot(all_wss$k,all_wss$twss)
lines(all_wss$twss)
```

If we are measuring for lowest variance of each grouping, then we want to find *k* where the line is close to the bottom of the graph but also where it is not too far to the right and therefore contains too many clusters. Afterall, having each polygon as its own cluster would explain 100% of the variance! We want to find the point where the variance is relatively low and where increasing numbers of *k* yeilds a diminishing return.

Finding the optimal number of clusters apparently cannot be found empirically so some of this is eye-balling. We see that our initial guess of 3 clusters is still on its way down. Looking at the slope, we might say that it looks noticebly steeper between 4 and 5 than 5 and six. This means that 5 or 6 clusters seems optimal.


```{r}
km <- kmeans(st_coordinates(st_centroid(airbnb)), centers = 5)
plot(st_geometry(airbnb), col = km$cluster)
```

## 4.2 SKATER Algorithm

However, a k-means approach isn't often the best clustering algorithm for spatial data and is best for tabular data. There are always ways to spatially constrain K-means on a variable of interest but it has no good implementation in R. More spatially explicit algorithms such as the SKATER algorithm might be better suited to variable data and multivariate data.

We will be using the **rgeoda** package and its array of algorithms for the rest of the lesson. Here we can use our spatial weights directly and use sf objects.

The SKATER or Spatial C(K)luster Analysis by Tree Edge Removal will be our first stop.

First we will load the **rgeoda** package, generate a weights matrix and specify which variables we want to use to build clusters. In this case, we can investigate price per person and review ratings for various airbnbs within each neighborhood.

**Keep in mind that different weight matricies will produce different results.**

```{r}
library(rgeoda)

w <- queen_weights(airbnb) # creating a weight matrix
summary(w)

data <- airbnb[c('price_pp','rev_rating')]

data
```

Now we can quickly run the SKATER algorithm by providing the number of clusters first, our weight matrix and our data of interest as a data frame. Looking at the summary will give us a partial glimpse of cluster membership as well as variance information.

```{r}
airbnb_clusters <- skater(10, w, data)
airbnb_clusters
```

If we plot our ten clusters we get the following. Some of these clusters look small which indicates that maybe there are too many present.

```{r}
ggplot() +
  geom_sf(data = airbnb, aes(fill = factor(airbnb_clusters$Cluster))) +
  theme_void()
```

We can apply the same logic as before by building out a for loop to test different numbers of clusters. Here, we have more than one variable which might make the elbow method a bit tricky.

```{r}
all_var <- tibble() #create empty master list

for (k in seq(2,20,1)) { #iterating from 2 clusters to 20 clusters by a step of 1
  skater <- skater(k, w, data) # running skater with # of clusters, weights, and data
  wcss <- skater$`Within-cluster sum of squares` # store within cluster sum of squares
  summary <- tibble(k = k, wcss = wcss,variable = factor(colnames(data)[1:2])) 
  all_var <- bind_rows(all_var, summary) # store kth iteration's info into master list
}

ggplot(all_var, aes(x=k, y=wcss, color = variable)) +
  geom_point() +
  theme_classic()
```

There may be several ways around this but one piece of data that might be useful is the Total Within Sum of Squares which should give us a variance estimate across all our variables for each iteration. This will give us a sort of inverse graph where we have to decide where the total variance explained by clustering is relatively high but also where *k* is not too high.

```{r}
all_var <- tibble()

for (k in seq(2,20,1)) {
  skater <- skater(k, w, data)
  total_wcss <- skater$`Total within-cluster sum of squares` # record total within sum of squares
  summary <- tibble(k = k, total_wcss = total_wcss)
  all_var <- bind_rows(all_var, summary)
}

ggplot(all_var, aes(x=k, y=total_wcss)) +
  geom_point() +
  theme_classic()
```
The biggest difference seems to be between 6 and 7 before beginning to taper off. If we run skater with 7 clusters this time, we still have some small clusters but they seems pretty robust to different *k* values so they must be pretty unique.


```{r}
airbnb_clusters <- skater(7, w, data)

ggplot() +
  geom_sf(data = airbnb, aes(fill = factor(airbnb_clusters$Cluster))) +
  theme_void()
```

Clustering might be a quick way to investigate relationships in your spatial data and not an end in itself. In that case, we can use **tmap** or **leaflet** packages to create interactive maps to see how the clusters relate to real-world streets, parks, etc.

```{r}
add_clusters <- tibble(clusters = airbnb_clusters$Cluster)

airbnb_tmap <- airbnb %>%
  bind_cols(add_clusters)

library(tmap)

tmap_mode("view")

tm_shape(airbnb_tmap) +
  tm_polygons("clusters", palette = "RdYlBu", alpha = 0.5)
```

We might also want to group each cluster's data and transform it to provide re-scaled scores in order to see how the two variables stack up in each cluster. Now when you click on a cluster, it will provide a price score 1-10 and the review rating as a pop-up.

```{r}
analysis <- airbnb_tmap %>%
  group_by(clusters) %>%
  summarize(price = mean(price_pp), rating = round(mean(rev_rating)/10,1)) %>%
  mutate(price_score = 10 * round(price / max(airbnb$price_pp),1))

tm_shape(analysis) +
  tm_polygons("clusters", palette = "RdYlBu", popup.vars = c("price_score", "rating"), alpha = 0.5)
```


## 4.3 REDCAP Clustering
Unlike SKATER, REDCAP is a bottom-up procedure of finding clusters, agglomerating features into clusters rather than beginning with one total cluster and splitting it down the line. The code for this cluster algorithm is nearly the same as SKATER except for one major difference.

We have different options for how feature connect in the building of the tree: "firstorder-singlelinkage", "fullorder-completelinkage", "fullorder-averagelinkage","fullorder-singlelinkage", "fullorder-wardlinkage". Each will change the results of clustering.

```{r}
redcap_clusters <- redcap(4, w, data, "fullorder-completelinkage")

ggplot() +
  geom_sf(data = airbnb, aes(fill = factor(redcap_clusters$Clusters))) +
  theme_void()
```

We can already see that the clustering is actually more contiguous than SKATER and we do not have any singleton clusters...yet anyway. What we can do now is test multiple levels of *k* as well as different linkage regimes and plot the elbow graphs for all possibilities.

```{r}
linkages <- c("firstorder-singlelinkage", "fullorder-completelinkage", "fullorder-averagelinkage","fullorder-singlelinkage", "fullorder-wardlinkage")

all_runs <- tibble()

for (linkage in unique(linkages)){
  for (k in seq(2,20)){
    redcap_clusters <- redcap(k, w, data, linkage)
    this_run <- tibble(community = airbnb$community,
                       k = k,
                       linkage = linkage,
                       twcss = redcap_clusters$`Total within-cluster sum of squares`,
                       cluster = factor(redcap_clusters$Clusters))
    all_runs <- bind_rows(all_runs, this_run)
  }
}

airbnb_redcap <- airbnb %>%
  left_join(all_runs)

ggplot(all_runs) +
  geom_point(aes(x = k, y = twcss)) +
  facet_wrap(~linkage)
```

Reading this many graphs isn't entirely straight forward but we can see the difference and how all but the last have obvious elbows! We can also use **facet_wrap** on our plots. Let's choose seven clusters as many of the plots seem to bend there.

We can see that many of the plots look a lot like our SKATER results but "fullorder-completelinkage" looks interesting of gives us at least one more larger cluster. We will choose this one.

```{r}
ggplot(airbnb_redcap %>% filter(k == 7)) +
  geom_sf(aes(fill = cluster)) +
  facet_wrap(~linkage)
```

Lastly, we can make a new object filtering for our choice of *k* and linkage and plot our final result.

```{r}
redcap_7 <- airbnb_redcap %>%
  filter(k == 7,linkage == "fullorder-completelinkage" )

ggplot(redcap_7) +
  geom_sf(aes(fill = cluster)) +
  theme_void()
```

## Conclusion

In this lesson, we saw how building for loops could enhance any cluster algorithm attempts by allowing us to test all numbers of clusters in a range, even different ways to construct clusters, and make an informed (or at least transparent) decision. Ultimately, whichever parameters you choose will need to be justified but they are also dependent on the data and what you are looking for. Sometimes intuition is important and knowing the data/place could sway your decision beyond the empricial measures of variance.

In addition, for loops are best built within the environment of scripts as we can construct them line-by-line, run them, modify them, and even copy much of the code from previous blocks to run new cluster algorithms. Scripts and loops can be used to construct entire spatial workflows.

In the next lesson, we will explore scripting in Python within QGIS and how GIS software help you to build scripts for the same purposes. We will be able to transfer some of our **R** knowledge to Python scripting which is what all GIS software is built upon. 
